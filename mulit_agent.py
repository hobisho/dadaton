# app.py
import os
import re
import base64
from typing import Optional
import requests
from PIL import Image
from io import BytesIO
import io
import torch
from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles
from starlette.responses import FileResponse 
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from transformers import AutoTokenizer, AutoModel
from transformers import BlipProcessor, BlipForConditionalGeneration
from langchain_community.vectorstores import Chroma
from langchain_core.embeddings import Embeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
import binascii
from  edge_tts import Communicate
from langdetect import detect
from translate import Translator


TEXT_FOLDER = "mydata"
IMAGE_FOLDER = "images"
MODEL_NAME = "llama3.2:1b"
STABLE_DIFFUSION_URL = "http://127.0.0.1:7860"
OLLAMA_URL = "http://localhost:11434"
EMBEDDING_MODEL = "intfloat/multilingual-e5-small"

app = FastAPI()
app.mount("/assets", StaticFiles(directory="harmonycarebu/dist/assets"), name="static")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

@app.get("/")
async def read_index():
    return FileResponse('harmonycarebu/dist/index.html')

class CustomE5Embedding(Embeddings):
    def __init__(self, model_path=EMBEDDING_MODEL):
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModel.from_pretrained(model_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        self.instruction = "query: "

    def embed_documents(self, texts):
        return [self.embed_query(t) for t in texts]

    def embed_query(self, text):
        formatted_text = self.instruction + text
        inputs = self.tokenizer(formatted_text, return_tensors="pt", truncation=True, padding=True).to(self.device)
        with torch.no_grad():
            model_output = self.model(**inputs)
        embeddings = model_output.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()
        return embeddings.tolist()

def query_ollama(prompt, model=MODEL_NAME):
    url = f"{OLLAMA_URL}/api/generate"
    headers = {"Content-Type": "application/json"}
    data = {"model": model, "prompt": prompt, "stream": False}
    response = requests.post(url, headers=headers, json=data)
    if response.ok:
        return response.json().get("response")
    else:
        print("Ollama éŒ¯èª¤ï¼š", response.text)
        return "æœ¬åœ°æ¨¡å‹ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹æª¢æŸ¥æ˜¯å¦æœ‰å•Ÿå‹•ã€‚"

def generate_image(prompt):
    payload = {
        "prompt": prompt,
        "steps": 20,
        "cfg_scale": 12,
        "width": 512,
        "height": 512,
    }
    try:
        response = requests.post(f"{STABLE_DIFFUSION_URL}/sdapi/v1/txt2img", json=payload)
        r = response.json()
        if 'images' in r:
            return r['images'][0]
        else:
            return None
    except Exception as e:
        print("åœ–ç‰‡ç”ŸæˆéŒ¯èª¤ï¼š", e)
        return None

def extract_image_prompt(text):
    match = re.search(r"\s*Image\s*Prompt\s*:\s*(.+)", text, re.IGNORECASE | re.DOTALL)
    print("match:", match)
    return match.group(1).strip() if match else None

blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(
    torch.device("cuda" if torch.cuda.is_available() else "cpu")
)

def is_valid_base64_image(data: str) -> bool:
    try:
        base64.b64decode(data, validate=True)
        return True
    except binascii.Error:
        return False
    
def get_image_caption(base64_str):
    try:
        image = Image.open(BytesIO(base64.b64decode(base64_str))).convert('RGB')
        inputs = blip_processor(image, return_tensors="pt").to(blip_model.device)
        with torch.no_grad():
            out = blip_model.generate(**inputs)
        caption = blip_processor.decode(out[0], skip_special_tokens=True)
        return caption
    except Exception as e:
        print("åœ–åƒæè¿°éŒ¯èª¤ï¼š", e)
        return "ç„¡æ³•è¾¨è­˜åœ–ç‰‡å…§å®¹"

# def get_voice(lang_code: str) -> str:
#     voice = {
#         "zh-tw": "zh-TW-XiaoxiaoNeural",
#         "en" : "en-US-JennyNeural",
#         "id" : "id-ID-DianNeural",
#         "vi" : "vi-VN-HaNeural",
#     }
#     lang_code = lang_code.lower()
#     if lang_code.startswith("zh"):
#         return voice["zh-tw"]
#     elif lang_code.startswith("en"):
#         return voice["en"]
#     elif lang_code.startswith("id"):
#         return voice["id"]
#     elif lang_code.startswith("vi"):
#         return voice["vi"]
#     else:
#         return voice["zh-tw"]

# === å¤šä»£ç†è¨­å®š ===
AGENT_FILES = {
    "elderly_recipes": {
        "file": "è€äººé£Ÿè­œ_RAGæ ¼å¼_å«æ•ˆæœæ¨™ç±¤.txt",
        "desc": "Nutritional meal plans for the elderly, dietary restrictions, and cooking recommendations suitable for patients, including explanations of ingredient benefits.",
    },
    "sea_culture": {
        "file": "æ±å—äºæ–‡åŒ–é¢¨ä¿—_RAGæ ¼å¼.txt",
        "desc": "Insights into the culture, taboos, traditions, and religious beliefs of Southeast Asian countries, such as Vietnam and Indonesia.",
    },
    "care_vocab": {
        "file": "æ±å—äºç…§é¡§è©å½™è³‡æ–™.txt",
        "desc": "Comparison of frequently used care vocabulary in the field, featuring Chinese-Vietnamese and Chinese-Indonesian equivalents to facilitate communication between caregivers and the elderly.",
    },
    "rehab_guide": {
        "file": "å¾©å¥å‹•ä½œ_RAGæ ¼å¼_åŠ å¼·ç‰ˆ.txt",
        "desc": "Instructions and guidelines for common rehabilitation exercises, including details and precautions for post-stroke, dementia, and strength training movements.",
    },
    "migrant_guide": {
        "file": "å¤–ç±ç§»å·¥é ˆçŸ¥_RAGæ ¼å¼.txt",
        "desc": "Guidelines and regulations for foreign caregivers working and living in Taiwan, including information on hiring, accommodation, and lifestyle rules.",
    },
    "general": {
        "file": None,
        "desc": "Handles everyday conversations or topics without a specific theme, such as greetings or simple chats.",
    },
}

embedding_function = CustomE5Embedding()
retrievers = {}

def init_retrievers():
    for key, meta in AGENT_FILES.items():
        file = meta["file"]
        if not file:
            continue
        with open(os.path.join(TEXT_FOLDER, file), encoding="utf-8") as f:
            text = f.read()
        chunks = re.split(r"\n{2,}|---+", text)
        docs = [p.strip() for p in chunks if p.strip()]
        splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)
        docs = splitter.create_documents(docs)
        vectorstore = Chroma.from_documents(docs, embedding=embedding_function)
        retrievers[key] = vectorstore.as_retriever()

print("ğŸ“„ è¼‰å…¥å¤šä»£ç†è³‡æ–™ä¸­...")
init_retrievers()
translator= Translator(from_lang="zh", to_lang="en")

class QueryInput(BaseModel):
    query: str
    image: Optional[str] = None

@app.post("/ask")
async def ask(input: QueryInput):
    query = input.query.strip().lower()
    simple_greetings = ["hi", "hello", "ä½ å¥½", "å—¨", "å“ˆå›‰", "hey", "halo"]
        
    mention_image = any(word in query for word in ["åœ–ç‰‡", "åœ–åƒ", "ç•«", "ç¹ªè£½", "ç”Ÿæˆåœ–", "ç•«é¢", "åœ–","ç…§ç‰‡","image","picture"])
    print("mention_image:",mention_image)
    image_caption = None
    if input.image and is_valid_base64_image(input.image):
        image_caption = get_image_caption(input.image)
        translator_T= Translator(from_lang="en", to_lang="zh")
        print("ğŸ“· åœ–åƒæè¿°ï¼š", image_caption)
        query += f"é€™å¼µåœ–ç‰‡çš„å…§å®¹æ˜¯ï¼š{ translator_T.translate(image_caption)}\n\nä½¿ç”¨è€…å•é¡Œï¼š{query}"
    elif input.image:
        print("âš ï¸ åœ–åƒè³‡æ–™ç„¡æ•ˆï¼Œå·²å¿½ç•¥")

    if query in simple_greetings:
        return {"answer": "ä½ å¥½ï¼Œæˆ‘æ˜¯ Harmonycareï¼Œæœ‰éœ€è¦å¹«å¿™ç…§é¡§é•·è¼©æˆ–æ–‡åŒ–æºé€šçš„å•é¡Œéƒ½å¯ä»¥å•æˆ‘å–”ï¼", "image": None}

    agent_descriptions = "\n".join([f"{key}: {meta['desc']}" for key, meta in AGENT_FILES.items() if key != "general"])
    agent_select_prompt = f"""
        ä½ æ˜¯ä¸€å€‹åˆ†é¡åŠ©æ‰‹ï¼Œåªè² è²¬åˆ¤æ–·å•é¡Œå±¬æ–¼å“ªä¸€å€‹ä¸»é¡Œé ˜åŸŸï¼Œä¸¦é¸åƒè€ƒæè¿°é¸æ“‡ä¸€å€‹æœ€é©åˆå¾—keyã€‚è«‹å‹™å¿…**åªå›å‚³ä¸‹åˆ— key åç¨±ä¸­çš„ä¸€å€‹ï¼Œä¸å¯è¼¸å‡ºèªªæ˜ã€æ¨™é»ã€é‡è¤‡æè¿°æˆ–å¤šå€‹ key**ã€‚

        === å¯é¸ key èˆ‡ä¸»é¡Œæè¿° ===
        {agent_descriptions}

        æœ‰çš„keyåç¨±:
        "migrant_guide"
        "elderly_recipes"
        "sea_culture"
        "care_vocab"
        "rehab_guide"
        "general"
        
        ç¾åœ¨è«‹åˆ¤æ–·ä»¥ä¸‹å•é¡Œè¦ä½¿ç”¨å“ªå€‹agentï¼Œç„¶å¾Œå›å‚³ä¸Šè¿°ä¸­å…­å€‹keyä¸­æœ€è²¼è¿‘å…§å®¹çš„ä¸€å€‹keyï¼Œ(åªè¦åç¨±ä¸è¦å…¶ä»–æè¿°)ï¼š
        ä½¿ç”¨è€…å•é¡Œï¼š{query}
        
        è«‹ä½ ä¸€å®šåªè¼¸å‡ºä¸Šè¿° key ä¸­çš„ä¸€å€‹ï¼Œ**å®Œå…¨ä¸è¦è¼¸å‡ºä»»ä½•å…¶ä»–æ–‡å­—ã€èªªæ˜ã€ç¬¦è™Ÿæˆ–æ¨™é»ç¬¦è™Ÿ**ï¼Œå¦å‰‡æœƒå°è‡´éŒ¯èª¤ã€‚
        """
    selected_agent_raw = query_ollama(agent_select_prompt)
    print("ğŸ¤– é¸æ“‡çš„ä»£ç†ï¼š", selected_agent_raw)

    valid_keys = list(AGENT_FILES.keys())
    match = re.search(r'\b(' + '|'.join(valid_keys) + r')\b', selected_agent_raw.lower())
    selected_agent = match.group(1) if match else "general"

    print("ğŸ¤– ä¿®æ­£å¾Œä»£ç† keyï¼š", selected_agent)
    context = ""
    if selected_agent != "general":
        docs = retrievers[selected_agent].get_relevant_documents(query)
        context = "\n\n".join([d.page_content for d in docs])

    
    # å…±ç”¨éƒ¨åˆ†ï¼šèƒŒæ™¯èˆ‡ä»‹ç´¹
    base_prompt = f"""
    ä½ å« Harmonycareï¼Œæ˜¯ä¸€ä½å°ˆæ¥­çš„è€äººç…§è­·èˆ‡æ–‡åŒ–æºé€šåŠ©æ‰‹ï¼Œå¹«åŠ©å¤–ç±çœ‹è­·èˆ‡å°ç£è€äººæ›´å¥½åœ°æºé€šèˆ‡ç…§é¡§ã€‚
    ä½ æ“æœ‰è·¨èªè¨€ã€è·¨æ–‡åŒ–èˆ‡é•·ç…§çŸ¥è­˜ï¼Œç†Ÿæ‚‰å„ç¨®å¾©å¥å‹•ä½œã€ç–¾ç—…é£²é£Ÿèˆ‡æ–‡åŒ–ç¦å¿Œã€‚
    è«‹ä¾ç…§ä½¿ç”¨è€…çš„éœ€æ±‚å›ç­”å•é¡Œï¼Œä¸¦æ ¹æ“šä¸‹æ–¹è³‡æ–™ {"èˆ‡åœ–ç‰‡å…§å®¹" if input.image and is_valid_base64_image(input.image) else ""} ä¾†è£œå……èªªæ˜æˆ–æä¾›å…·é«”å»ºè­°ã€‚
    ä»¥ä¸‹æ˜¯å¯åƒè€ƒçš„ç…§è­·èˆ‡æ–‡åŒ–è³‡æ–™ï¼š
    {context}
    """

    # å¦‚æœæœ‰æœ‰æ•ˆåœ–ç‰‡ï¼Œå‰‡åŠ å…¥åœ–ç‰‡æè¿°
    image_section = ""
    if input.image and is_valid_base64_image(input.image):
        image_section = "ä»¥ä¸‹æ˜¯ä½¿ç”¨è€…ä¸Šå‚³çš„åœ–ç‰‡å…§å®¹æè¿°ï¼š" + image_caption + "\n\n"

    # åŠ å…¥ä½¿ç”¨è€…å•é¡Œ
    question_section = f"ä½¿ç”¨è€…å•é¡Œï¼š{query}\n\n"

    # é è¨­å›ç­”è¦æ±‚ï¼ˆæ¢åˆ—å¼é‡é»ï¼‰
    answer_instruction = "è«‹è¼¸å‡ºæ¢åˆ—å¼é‡é»ï¼Œå…§å®¹è¦ç°¡æ½”æ˜ç¢ºï¼Œåªéœ€è¼¸å‡ºä¸€å€‹æœ€ä½³ç­”æ¡ˆï¼Œé¿å…åˆ—å‡ºå¤šå€‹ç‰ˆæœ¬æˆ–æ€è€ƒéç¨‹ã€‚\n\n"

    # æ±ºå®šæ˜¯å¦éœ€è¦è¦æ±‚è¼¸å‡ºåœ–ç‰‡æè¿°ï¼šåªæœ‰ç•¶æŸ¥è©¢ä¸­æåŠåœ–ç‰‡ï¼Œä¸”(è‹¥æœ‰ generate_image åƒæ•¸å‰‡ç‚º True)æ™‚ï¼Œæ‰é™„åŠ åœ–ç‰‡æè¿°æŒ‡ç¤º
    image_prompt_instruction = ""
    if mention_image and (not hasattr(input, "generate_image") or input.generate_image):
        image_prompt_instruction = (
            "å› ç‚ºä½¿ç”¨è€…æƒ³è¦ç”Ÿæˆåœ–ç‰‡ï¼Œæ‰€ä»¥è«‹ä»¥åƒè€ƒçŸ¥è­˜è³‡æ–™ç”Ÿæˆä¸€ä»½è‹±æ–‡çš„åœ–ç‰‡æè¿°ï¼Œé€™ä»½æè¿°æ˜¯ç‚ºäº†è®“ç”Ÿåœ–æ¨¡å‹èƒ½å¤ æ›´åŠ äº†è§£é€™å€‹åœ–ç‰‡æœ‰ä»€éº¼å…§å®¹"
            "é‡è¦!!!  \n é€™ä»½è‹±æ–‡æè¿°ï¼Œè«‹ä¸€å®šè¦éµå¾ªä»¥ä¸‹æ ¼å¼ï¼ˆåƒ…ä¸€æ¬¡ï¼‰ï¼š\n"
            "Image Prompt :"
            "< \"......\">\n"
            "è«‹ä¸è¦è¼¸å‡ºå¤šçµ„ Image Prompt: æˆ–å…¶ä»–å‚™è¨»ã€‚\n"
        )

    # çµ„åˆå®Œæ•´ prompt
    full_prompt = base_prompt + image_section + question_section + answer_instruction + image_prompt_instruction

    # å‘¼å«æ¨¡å‹å–å¾—å›ç­”
    answer = query_ollama(full_prompt)
    print("ğŸ§  æ¨¡å‹å›ç­”ï¼š", answer)

    img_prompt = extract_image_prompt(answer) if mention_image else None
    translator= Translator(from_lang="zh", to_lang="en")
    
    print("img_prompt:", img_prompt)
    print("query:", query)
    
    image_base64 = generate_image(translator.translate(img_prompt)  if img_prompt is not None else  translator.translate(query)) if (img_prompt or mention_image) else None

    # if image_base64:
    #     print("ğŸ–¼ï¸ å·²ç”¢ç”Ÿåœ–ç‰‡ (base64)")
    # elif mention_image:
    #     print("âš ï¸ æåŠåœ–ç‰‡ä½†æœªæˆåŠŸç”¢ç”Ÿï¼Œå¯èƒ½æ˜¯æ¨¡å‹æœªæä¾›æè¿°")
    # lang = detect(answer)
    # voice = get_voice(lang)
    # print("èªéŸ³é¸æ“‡ï¼š", voice) 
    # communicate = Communicate(answer, voice=voice)
    # stream = io.BytesIO()
    # async for chunk in communicate.stream():
    #     if chunk["type"] == "audio":
    #         stream.write(chunk["data"])
    # stream.seek(0)
    
    return {"answer": answer, "image": image_base64}
    # return {"answer": answer, "image": image_base64, StreamingResponse: StreamingResponse(stream, media_type="audio/mpeg")}

# @app.post("/tts")
# async def tts(text: str):
#     lang = detect(text)
#     voice = get_voice(lang)
#     print("èªéŸ³é¸æ“‡ï¼š", voice)
#     communicate = Communicate(text, voice=voice)
#     stream = io.BytesIO()
#     async for chunk in communicate.stream():
#         if chunk["type"] == "audio":
#             stream.write(chunk["data"])
#     stream.seek(0)
#     return StreamingResponse(stream, media_type="audio/mpeg")
    
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
